# -*- coding: utf-8 -*-
"""Emotion Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-aAWZIm1zLps7vO8rQkb8Enpetx1e64P
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
# % matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow
print(tensorflow.__version__)

import tensorflow as tf
tf.test.gpu_device_name()

#importing all the necessary packages
#tensorflow 2.0
from tensorflow.keras import datasets, layers, models

data = pd.read_csv('drive/My Drive/DATA files/Face Emotion detection/fer2013.csv')

data.info()

data.head()

print(data.emotion.unique()) #there are totally seven emotions

# getting pixels data from the Dataframe
def extract_from_pixelrow(pixels):
    pixels = pixels.split(' ')
    pixels = np.array([int(i) for i in pixels])
    return np.reshape(pixels, (48, 48))    

def extract_image(pixels):
    pixels = pixels.as_matrix()[0] # The output is a string
    return extract_from_pixelrow(pixels)

def overview(total_rows, data):
    fig = plt.figure(figsize=(10,10))
    idx = 0
    for i, row in data.iterrows():
        input_img = extract_from_pixelrow(row.pixels)
        ax = fig.add_subplot(5,5,idx+1)
        ax.imshow(input_img, cmap=plt.cm.gray)
        plt.xticks(np.array([]))
        plt.yticks(np.array([]))
        plt.tight_layout()
        idx += 1
    plt.show()

num_images = 25
data = data.sample(n=num_images)
overview(num_images, data) # overview of face data as thumbnails (private)

emotion_description = {0:"Angry", 1:"Disgust", 2:"Fear", 3:"Happy", 4:"Sad", 5:"Surprise", 6:"Neutral"}

with open("drive/My Drive/DATA files/Face Emotion detection/fer2013.csv") as f:
    content = f.readlines()

lines = np.array(content)

num_of_instances = lines.size
print("number of instances: ",num_of_instances)
print("instance length: ",len(lines[1].split(",")[1].split(" ")))

# Commented out IPython magic to ensure Python compatibility.
#initialize trainset and test set
train_images, train_labels, test_images, test_labels = [], [], [], []

#transfer train and test set data
for i in range(1,num_of_instances):
    try:
        emotion, img, usage = lines[i].split(",")
          
        val = img.split(" ")
            
        pixels = np.array(val, 'float32')
        
        emotion = tf.keras.utils.to_categorical(emotion, num_classes=7)
    
        if 'Training' in usage:
            train_labels.append(emotion)
            train_images.append(pixels)
        elif 'PublicTest' or 'PrivateTest' in usage:
            test_labels.append(emotion)
            test_images.append(pixels)
    except:
      print("",end="")

#data transformation for train and test sets
train_images = np.array(train_images, 'float32')
train_labels = np.array(train_labels, 'float32')
test_images = np.array(test_images, 'float32')
test_labels = np.array(test_labels, 'float32')

train_images /= 255 #normalize inputs between [0, 1]
test_images /= 255

train_images = train_images.reshape(train_images.shape[0], 48, 48, 1)
train_images = train_images.astype('float32')
test_images = test_images.reshape(test_images.shape[0], 48, 48, 1)
test_images = test_images.astype('float32')

print(train_images.shape[0], 'train samples')
print(test_images.shape[0], 'test samples')
# %time

train_images.shape

test_images.shape

train_labels.shape

test_labels.shape

model = models.Sequential()

#1st layer

model.add(layers.Conv2D(64, (3, 3), padding='same', input_shape=(48, 48, 1)))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.25))

#2nd layer

model.add(layers.Conv2D(128, (3, 3), padding='same'))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.25))

#3rd layer

model.add(layers.Conv2D(512, (3, 3), padding='same'))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.25))

#4th layer

model.add(layers.Conv2D(1024, (3, 3), padding='same'))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.25))

model.add(layers.Flatten())

#fully connected 1st layer

model.add(layers.Dense(256))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dropout(0.25))

#fully connected 2nd layer

model.add(layers.Dense(512))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dropout(0.25))

#fully connected 3rd layer

model.add(layers.Dense(512))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dropout(0.5))

model.add(layers.Dense(7, activation='softmax'))

from tensorflow.keras.optimizers import Adam

opt=Adam(lr=0.0001)

model.compile(optimizer = opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#Image data Generator
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint("drive/My Drive/DATA files/Face Emotion detection/model1_weights_emotion.h5", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

datagen = ImageDataGenerator(zoom_range=0.2, horizontal_flip=True)

# fits the model on batches with real-time data augmentation:
history=model.fit_generator(datagen.flow(train_images, train_labels, batch_size=128),
                    steps_per_epoch=300, epochs=100,validation_data=(test_images, test_labels),callbacks=callbacks_list)

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

model.save('model_final_weights_emotion.h5')
model_file = drive.CreateFile({'CNN_model' : 'model_final_weights_emotion.h5'})
model_file.SetContentFile('model_final_weights_emotion.h5')                      
model_file.Upload()

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.1, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

load_model = tf.keras.models.load_model('drive/My Drive/model_final_weights_emotion.h5')

load_model.summary()

y_prob = load_model.predict(test_images)
y_pred = np.argmax(y_prob)
y_actual = np.argmax(test_labels)

"""**Testing liera**"""

image_test= tf.keras.preprocessing.image.load_img('drive/My Drive/RDJ_happy.png', grayscale=False,
    color_mode='rgb')

img_test3 = tf.keras.preprocessing.image.load_img('drive/My Drive/RDJ_happy.png',
    color_mode='grayscale',
    target_size=(48,48))
img1 = np.array(img_test3)
img1.shape
img1 = img1 / 255.0
img1 = img1.reshape(1, 48, 48, 1)

labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise','neutral']

result1 = np.argmax(load_model.predict(img1))
print(labels[result1])

plt.figure(figsize=(10,10))
plt.imshow(image_test)
plt.xlabel(labels[result1])
plt.show()

image_test1= tf.keras.preprocessing.image.load_img('drive/My Drive/Mona-Lisa.jpg', grayscale=False,
    color_mode='rgb')
img_test4 = tf.keras.preprocessing.image.load_img('drive/My Drive/Mona-Lisa.jpg',
    color_mode='grayscale',
    target_size=(48,48))
img2 = np.array(img_test3)
img2.shape
img2 = img2 / 255.0
img2 = img2.reshape(1, 48, 48, 1)
result2 = np.argmax(load_model.predict(img2))
print(labels[result2])
plt.figure(figsize=(10,10))
plt.imshow(image_test1)
plt.xlabel(labels[result2])
plt.show()

image_test2= tf.keras.preprocessing.image.load_img('drive/My Drive/test1.jpg', grayscale=False,
    color_mode='rgb')
img_test5 = tf.keras.preprocessing.image.load_img('drive/My Drive/test1.jpg',
    color_mode='grayscale',
    target_size=(48,48))
img3 = np.array(img_test5)
img3.shape
img3 = img3 / 255.0
img3 = img3.reshape(1, 48, 48, 1)
result3 = np.argmax(load_model.predict(img3))
print(labels[result3])
plt.figure(figsize=(10,10))
plt.imshow(image_test2)
plt.xlabel(labels[result3])
plt.show()

"""emotion_dict = {0: "Angry", 1: "Disgust", 2: "Fear", 3: "Happy", 4: "Sad", 5: "Surprise", 6: "Neutral"}

model_rt = load_model

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()

    if ret is True:
      gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    else:
      break
    
    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)

    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)
        roi_gray = gray[y:y + h, x:x + w]
        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)
        cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)
        prediction = model_rt.predict(cropped_img)
        cv2.putText(frame, emotion_dict[int(np.argmax(prediction))], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)

    cv2.imshow('frame', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()"""